{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/UNSW-COMP9418/Week07/blob/main/COMP9418_W07_Markov_Netwoks_and_Conditional_Random_Fields_Solutions.ipynb)\n",
    "\n",
    "# Markov Networks and Conditional Random Fields\n",
    "\n",
    "**COMP9418 W07 Tutorial**\n",
    "\n",
    "- Instructor: Gustavo Batista\n",
    "- School of Computer Science and Engineering, UNSW Sydney \n",
    "- Notebook designed by Gustavo Batista and Jeremy Gillen\n",
    "- Last Update 6th September 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this week's tutorial, we will explore the class of undirected graphs known as *Markov networks* or *Markov Random Fields*. These networks are used to model symmetrical dependencies between random variables. These models are particularly popular in areas such as image and language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical prerequisites\n",
    "\n",
    "You will need certain packages installed to run this notebook.\n",
    "\n",
    "If you are using ``conda``'s default\n",
    "[full installation](https://conda.io/docs/install/full.html),\n",
    "these requirements should all be satisfied already.\n",
    "\n",
    "If you are using ``virtualenv`` or other native package management,\n",
    "you may need to run these commands:\n",
    "\n",
    "```python\n",
    "pip3 install numpy matplotlib\n",
    "```\n",
    "\n",
    "Once you have done all that, you can import some useful modules for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Libraries for creating and assessing machine learning classifiers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# combinatorics\n",
    "from itertools import product, combinations\n",
    "# clear display\n",
    "from IPython import display\n",
    "# access to math.log2\n",
    "import math\n",
    "# random number generator\n",
    "import random as rnd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DiscreteFactors import Factor\n",
    "from Graph import Graph\n",
    "from BayesNet import BayesNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image smoothing\n",
    "Let's start illustrating how Markov networks work with image smoothing. Remember that image smoothing is the process of removing random noise from an image. For instance, pictures taken in low illumination conditions frequently appear *pixelated*.\n",
    "\n",
    "We start loading one image we will use to test our Markov network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_noisy=mpimg.imread('img/05_25_noisy.png')\n",
    "plt.imshow(img_noisy, cmap='gray')\n",
    "(size_x, size_y) = img_noisy.shape\n",
    "print(\"Image size:\")\n",
    "print(size_x, \"x\", size_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this is a small 256 x 256 grayscale image. The colour values are numbers between 0 and 255. Even for a small image like this, we need to be careful with our implementation. Otherwise, we can end up with a code that is too slow or use too much memory. Our Markov networks will use the following graph structure.\n",
    "\n",
    "![](./img/image_graph.png)\n",
    "\n",
    "Where $Y_{i,j}$ is a pixel in the input (noisy) image and $X_{i,j}$ is a pixel in the output (filtered) image. To represent both images, we will use 2-dimensional matrices (256 x 256) instead of factors.\n",
    "\n",
    "Also, our model will have two forms of pairwise potentials:\n",
    "1. $\\psi(X_{i,j}, X_{i',j'})$ represents the correlations between neighbouring pixels.\n",
    "2. $\\psi(X_{i,j}, Y_{i,j})$ describes the correlations between pixels in the same position in the noisy and filtered images.\n",
    "\n",
    "The final component of our model is the energy function. In general, the energy function has the following format:\n",
    "\n",
    "$$E(\\textbf{X})=\\sum_{c\\in Cliques(G)}\\psi_c(\\textbf{X}_c)$$\n",
    "\n",
    "Let's instantiate this energy function to our problem. Our cliques are pairwise connections between pixels. We will assume the neighboring pixes are in the positions $(i-1, j), (i+1, j), (i, j-1), (i, j+1)$. Therefore, we have:\n",
    "\n",
    "$$E(\\textbf{X})=\\sum_{i,j}\\psi(X_{i,j}, Y_{i,j})+\\psi(X_{i,j}, X_{i+1,j})+\\psi(X_{i,j}, X_{i,j+1})$$\n",
    "\n",
    "\n",
    "## Stochastic Search\n",
    "\n",
    "We will now implement the image smoothing using Markov networks. We will develop a stochastic search algorithm, as described in the lecture.\n",
    "\n",
    "This algorithm will choose one pixel and change its value. If such a change decreases the total image energy, we will accept the change. Otherwise, the change will be ignored, and we move to another pixel. Besides, the algorithm will accept changes that increase the energy with a small probability of $p$ in an attempt to avoid local minima.\n",
    "\n",
    "Remember that our objective is to compute an MPE query (we are interested in the filtered image with the highest probability/smallest energy). Therefore, the stochastic search algorithm will move towards a (local) minima through a series of changes in the pixel values.\n",
    "\n",
    "To improve the code efficiency, we note that a change in a single pixel can only affect the pixel neighbourhood. Therefore, we will only compute the following function to assess the difference in the energy function:\n",
    "\n",
    "$$\\Delta E(\\textbf{X})=\\psi(X_{i,j}, Y_{i,j})+\\psi(X_{i,j}, X_{i+1,j})+\\psi(X_{i,j}, X_{i,j+1})+\\psi(X_{i,j}, X_{i-1,j})+\\psi(X_{i,j}, X_{i,j-1})$$\n",
    "\n",
    "Let's start defining some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This helper function converts a number between 0 and x*y into an (x,y) coordinate\n",
    "def int_to_coord(pos, size_y):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `pos`, a position expressed as an integer number between 0 and image's size_x * size_y\n",
    "    `size_y`, number of columns of the image matrix\n",
    "    \n",
    "    Returns pos converted into a (x, y) coordinate\n",
    "    \"\"\"\n",
    "    return int(pos / size_y), pos % size_y\n",
    "\n",
    "# This helper function returns a list with the neighbours of a (x, y) pixel respecting the image limits\n",
    "def neighbours(x, y, size_x, size_y):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `x, y`, pixel coordinate\n",
    "    `size_x, size_y`, image size\n",
    "    \n",
    "    Returns a list with valid (x, y) neighbours\n",
    "    \"\"\"\n",
    "    \n",
    "    neighbours = list()\n",
    "    if x > 0:\n",
    "        neighbours.append((x-1, y))\n",
    "    if x < size_x - 1:\n",
    "        neighbours.append((x+1, y))\n",
    "    if y > 0:\n",
    "        neighbours.append((x, y-1))\n",
    "    if y < size_y - 1:\n",
    "        neighbours.append((x, y+1))\n",
    "    return neighbours\n",
    "\n",
    "# This helper function defines energy between two data points\n",
    "def energy_point(x, y):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `x, y`, pixel color [0,255] range\n",
    "\n",
    "    Returns the absolute difference between x and y. Other measures of dissimilarity may also work such as (x - y)**2\n",
    "    \"\"\"    \n",
    "    return abs(x - y)\n",
    "    \n",
    "# This helper function plots the results every 10 iterations so we can see the convergence\n",
    "def plot_results(it, image_x, image_y, energies, max_iter):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `it`, current iteraction\n",
    "    `image_x`, filtered image\n",
    "    `image_y`, observed image\n",
    "    `energies`, list of energy values computed for each iteration\n",
    "    `max_iter`, maximum number of iterations\n",
    "    \"\"\"    \n",
    "    \n",
    "    (size_x, size_y) = image_y.shape\n",
    "    energy = sum([energy_point(image_x[x][y], image_y[x][y]) for x in range(size_x) for y in range(size_y)])\n",
    "    energy += sum([energy_point(image_x[x][y], image_x[x+1][y]) for x in range(size_x-1) for y in range(size_y)])\n",
    "    energy += sum([energy_point(image_x[x][y], image_x[x][y+1]) for x in range(size_x) for y in range(size_y-1)])\n",
    "    energies.append(energy)               \n",
    "    display.clear_output(wait=True)\n",
    "    print(\"Iteration:\")\n",
    "    print(it)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([0, max_iter])\n",
    "    axes.set_ylim([min(max(energies)/2,min(energies)), max(energies)])\n",
    "    plt.plot(energies, 'ro')\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.imshow(image_y, cmap='gray') \n",
    "    plt.subplot(2, 2, 4)    \n",
    "    plt.imshow(image_x, cmap='gray') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Now, it is your turn. Let's implement a stochastic search. We have created a stub for you. You only need to fill in a few gaps. (This function will take a while, give it >20 seconds to produce output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_search(image_y, max_iter=101, eps=.001, step_stdev=.05):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `image_y`, observed image\n",
    "    `max_iter`, maximum number of iteractions\n",
    "    `eps`, probability of accepting a higher energy change\n",
    "    `step_stdev`, standard deviation. Changes are randon numbers from a Gaussian distribuion N(0, step_stdev)\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of energy values for each iteration. Initialise with empty list\n",
    "    energies = []\n",
    "    # Image size expressed as number of rows and columns\n",
    "    (size_x, size_y) = image_y.shape\n",
    "    # Image size expressed as number of pixels\n",
    "    size = size_x * size_y\n",
    "    # Let's use image_y as starting point for image_x. It will be faster than if we start with a random assignment\n",
    "    image_x = image_y.copy()\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # order has the indexes of all pixels in the image\n",
    "        order = list(range(size))\n",
    "        # Let's shuffle these indexes so each pass will use a different order\n",
    "        rnd.shuffle(order)\n",
    "        for o in order:\n",
    "            # Use the int_to_coord helper function to convert o into an (x,y) coordinate\n",
    "            (x, y) = ... # TODO                                                                       # 1 line\n",
    "            # Use energy_point to initialise energy_prev with the energy between image_x and image_y in the same pixel position (x, y)\n",
    "            energy_prev = ... # TODO                                                                  # 1 line\n",
    "            # step defines the change in image_x pixel. We will use a random value from a normal distribution with mean zero and a small standard deviation\n",
    "            step = rnd.gauss(0, step_stdev)\n",
    "            # Use energy_point to initialise energy_post with the energy between image_x +step and image_y in the same pixel position (x, y)\n",
    "            energy_post = ... # TODO                                                                  # 1 line\n",
    "            # Use neighbours to find all valid neighbours of the current pixel\n",
    "            for (i, j) in neighbours(x, y, size_x, size_y):\n",
    "                # Use energy_point to update energy_prev for each neighbour pixel\n",
    "                energy_prev += ... # TODO                                                             # 1 line\n",
    "                # Use energy_point to update energy_post for each neighbour pixel\n",
    "                energy_post += ... # TODO                                                             # 1 line\n",
    "            # Update image_x if the change led to an decrease of energy    \n",
    "            if energy_post < energy_prev or rnd.random() < eps:\n",
    "                image_x[x][y] += step\n",
    "        # Call plot_results to show the progress so far\n",
    "        plot_results(it, image_x, image_y, energies, max_iter)\n",
    "\n",
    "##################\n",
    "# Test code\n",
    "\n",
    "stochastic_search(img_noisy, max_iter=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "def stochastic_search(image_y, max_iter=101, eps=.001, step_stdev=.05):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `image_y`, observed image\n",
    "    `max_iter`, maximum number of iteractions\n",
    "    `eps`, probability of accepting a higher energy change\n",
    "    `step_stdev`, standard deviation. Changes are randon numbers from a Gaussian distribuion N(0, step_stdev)\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of energy values for each iteration. Initialise with empty list\n",
    "    energies = []\n",
    "    # Image size expressed as number of rows and columns\n",
    "    (size_x, size_y) = image_y.shape\n",
    "    # Image size expressed as number of pixels\n",
    "    size = size_x * size_y\n",
    "    # Let's use image_y as starting point for image_x. It will be faster than if we start with a random assignment\n",
    "    image_x = image_y.copy()\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # order has the indexes of all pixels in the image\n",
    "        order = list(range(size))\n",
    "        # Let's shuffle these indexes so each pass will use a different order\n",
    "        rnd.shuffle(order)\n",
    "        for o in order:\n",
    "            # Use the int_to_coord helper function to convert o into an (x,y) coordinate\n",
    "            (x, y) = int_to_coord(o, size_y)\n",
    "            # Use energy_point to initialise energy_prev with the energy between image_x and image_y in the same pixel position\n",
    "            energy_prev = energy_point(image_x[x][y], image_y[x][y])\n",
    "            # step defines the change in image_x pixel. We will use a random value from a normal distribution with mean zero and a small standard deviation\n",
    "            step = rnd.gauss(0, step_stdev)\n",
    "            # Use energy_point to initialise energy_post with the energy between image_x +step and image_y in the same pixel position\n",
    "            energy_post = energy_point(image_x[x][y] + step, image_y[x][y])\n",
    "            # Use neighbours to find all valid neighbours of the current pixel\n",
    "            for (i, j) in neighbours(x, y, size_x, size_y):\n",
    "                # Use energy_point to update energy_prev for each neighbour pixel\n",
    "                energy_prev += energy_point(image_x[x][y], image_x[i][j])\n",
    "                # Use energy_point to update energy_post for each neighbour pixel\n",
    "                energy_post += energy_point(image_x[x][y] + step, image_x[i][j])\n",
    "            # Update image_x if the change led to an decrease of energy    \n",
    "            if energy_post < energy_prev or rnd.random() < eps:\n",
    "                image_x[x][y] += step\n",
    "        # Call plot_results to show the progress so far\n",
    "        plot_results(it, image_x, image_y, energies, max_iter)\n",
    "\n",
    "##################\n",
    "# Test code\n",
    "\n",
    "stochastic_search(img_noisy, max_iter=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Random Fields (CRFs)\n",
    "\n",
    "Conditional Random Fields are *discriminative* graphical models. These models are a very popular way to integrate the output of multiple independent classifiers into a more complex structured prediction.\n",
    "\n",
    "A simple example is the classification of sequences. As the next figure illustrates:\n",
    "\n",
    "![](./img/chain_CRF.png)\n",
    "\n",
    "This is a *linear chain CRF* in which we want to predict an entire word given predictions for individual letters.\n",
    "\n",
    "In this part of the tutorial, we will implement this application of word prediction using Machine Learning classifiers trained to recognise individual letters.\n",
    "\n",
    "Let's start by reading the dataset. It is a large dataset of handwritten letters made available in a [Kaggle competition](https://www.kaggle.com/ponrajsubramaniian/az-handwritten-digits). \n",
    "\n",
    "This dataset may take a while to load. If your computer has a few GB of memory free, use the larger dataset 'data/A_Z_Handwritten_Data.csv'. If not, use the smaller dataset 'data/A_Z_Handwritten_Data_Small.csv', which is used by default below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt('data/A_Z_Handwritten_Data_Small.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a reasonably large set of handwritten characters, let's see how many examples and attributes we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(num_examples, num_attributes) = dataset.shape\n",
    "print(\"Number of examples\", num_examples)\n",
    "print(\"Number of attributes\", num_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first attribute is the class label, encoded as 0 = 'a', 1 = 'b', and so on. Let's remove this column from the data, so there will be no risk of incorrectly providing this information to the classifier as a regular feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:,1:785]\n",
    "Y = dataset[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the following helper function will be handy when we need to convert the class numbers 0-25 into letters a-z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_letter(n):\n",
    "    return string.ascii_lowercase[int(n)]\n",
    "\n",
    "################\n",
    "# Test code\n",
    "\n",
    "for i in range(26):\n",
    "    print(num_to_letter(i), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now split the dataset into training and test sets. We use a 50%-50% split since we have plenty of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, Y_train, Y_test) = train_test_split(X, Y, test_size=0.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is provided as a CSV file with 784 attributes. Now, we convert the data to 28 x 28-pixel images so that we can visualise part of the dataset. Notice that we created new variables `X_train_img` and `X_test_img` to preserve the original data for training and testing the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape images, from 1 x 784 to 28 x 28\n",
    "X_train_img = X_train.reshape(X_train.shape[0], 28, 28).astype('float32')\n",
    "X_test_img = X_test.reshape(X_test.shape[0], 28, 28).astype('float32')\n",
    "\n",
    "# The pixel values are numbers in the range 0..255. We need to convert into 0..1 range\n",
    "X_train_img = X_train_img / 255\n",
    "X_test_img = X_test_img / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell plots some random images so that we can have a better feeling of the appearance of the handwritten letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,10))\n",
    "row, colums = 4, 4\n",
    "for i in range(16):  \n",
    "    plt.subplot(colums, row, i+1)\n",
    "    plt.imshow(X_train_img[i],interpolation='nearest', cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning classifier for recognition of individual letters\n",
    "\n",
    "We have loaded and prepared our data. It is time to build some models. We can create any model we want, and you are invited to test different models later on.\n",
    "\n",
    "In this part of the tutorial, we will use Random Forests (RF) as classifiers for several reasons:\n",
    "\n",
    "1. They are simple to train. Although RFs have several hyperparameters, they often provide very good results with default parameter values.\n",
    "2. They are naturally multiclass. Since we have 26 classes, it is simpler to deal with a classifier that can handle all of them, instead of converting the data into multiple binary-class datasets.\n",
    "3. They frequently provide competitive classification accuracy compared to other state-of-the-art Machine Learning classifiers.\n",
    "\n",
    "A downside is that Random Forests are ensemble classifiers that typically use a few hundred trees. Therefore, they tend to be slow for training and classification. \n",
    "\n",
    "The next cell will build a model. Since we have a large dataset, we will limit the number of trees to 100. You can play with this and other settings later on. The next cell will take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=100)\n",
    "classifier.fit(X_train, Y_train)\n",
    "Y_pred = classifier.predict(X_test)\n",
    "print(confusion_matrix(Y_test, Y_pred))\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "print(accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inspect the performance results to check the classes that are frequently misclassified. We can also see that the overall performance is quite good, typically above 97% accuracy. (Around 90% for small dataset)\n",
    "\n",
    "## Making words\n",
    "\n",
    "Unfortunately, our dataset does not have words, only handwritten characters. Therefore, we will have to develop a series of functions to produce words from characters. We start with a helper function that creates a Python dictionary. The dictionary keys are letters (a-z), and the items are lists with the indexes of the handwritten characters in the *test* set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function, creates a python dictionary maping lowercase letters to indexes of examples in the test set\n",
    "def letter_idxs():\n",
    "    letters = {}\n",
    "    for i in range(26):\n",
    "        letters[num_to_letter(i)] = []\n",
    "    for i in range(len(Y_test)):\n",
    "        letters[num_to_letter(Y_test[i])].append(i)\n",
    "    return letters\n",
    "\n",
    "#################\n",
    "# Test code\n",
    "\n",
    "letters_idx_dict = letter_idxs()\n",
    "print(\"Some examples of letters 'a' in the test set\")\n",
    "plt.figure(figsize = (12,10))\n",
    "row, colums = 4, 4\n",
    "for i in range(16):  \n",
    "    plt.subplot(colums, row, i+1)\n",
    "    plt.imshow(X_test_img[letters_idx_dict['a'][i]],interpolation='nearest', cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the letters dictionary to create words composed by the handwritten characters in the test set. We will use these words to test our classifiers. The next helper function produces a sequence of character images given a string. We randomly pick a letter from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function converts an string into a sequence of indexes of test set examples. The indexes are randomly chosen by respect the letters in the string\n",
    "def make_word(word, letters, X_test):\n",
    "    '''\n",
    "    Returns a list of np arrays (images), one corresponding to each letter in the word\n",
    "    '''\n",
    "    letter_indicies = [rnd.choice(letters[char]) for char in word]\n",
    "    return [X_test[idx] for idx in letter_indicies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can test these function by printing a word such as \"bayes\". If you want to try different words, remember to use lowercase letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function prints a sequence of handwritten letters specified by word\n",
    "def print_word(word_imgs):\n",
    "    f, axarr = plt.subplots(1,len(word_imgs))\n",
    "    for i, letter_image in enumerate(word_imgs):\n",
    "        letter_image = np.reshape(letter_image, (28, 28))\n",
    "        axarr[i].imshow(letter_image,cmap='Greys')\n",
    "        axarr[i].axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "##################\n",
    "# Test code\n",
    "\n",
    "rnd.seed(1)\n",
    "letters_idx_dict = letter_idxs()\n",
    "word_imgs = make_word(\"bayes\", letters_idx_dict, X_test_img)\n",
    "print_word(word_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can classify a word using our Random Forest classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function classifies a sequence of handwritten letters using the RF classifier\n",
    "\n",
    "def classify_random_forest(word_imgs):\n",
    "    pred = classifier.predict([letter_image for letter_image in word_imgs])\n",
    "    pred_word = ''.join([num_to_letter(i) for i in pred])\n",
    "    return pred_word\n",
    "\n",
    "##################\n",
    "# Test code\n",
    "\n",
    "rnd.seed(1)\n",
    "word_imgs = make_word(\"bayes\", letters_idx_dict, X_test)\n",
    "print_word(word_imgs)\n",
    "print(\"Classification (Random Forest):\", classify_random_forest(word_imgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Factors\n",
    "\n",
    "Before we move on to making a Conditional Random Field classifier, lets make a class to represent logarithmic factors. These work the same as normal factors, except when we join factors we have to add the values instead of multiplying them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogFactor(Factor):\n",
    "    def join(self, other):\n",
    "        '''\n",
    "        Usage: `new = f.join(g)` where f and g are Factors\n",
    "        This function multiplies two factors.\n",
    "        '''\n",
    "        # confirm that any shared variables have the same outcomeSpace\n",
    "        for var in set(other.domain).intersection(set(self.domain)):\n",
    "            if self.outcomeSpace[var] != other.outcomeSpace[var]:\n",
    "                raise IndexError('Incompatible outcomeSpaces. Make sure you set the same evidence on all factors')\n",
    "        # extend current domain with any new variables required\n",
    "        new_dom = list(self.domain) + list(set(other.domain) - set(self.domain)) \n",
    "        # to prepare for multiplying arrays, we need to make sure both arrays have the correct number of axes\n",
    "        self_t = self.table\n",
    "        other_t = other.table\n",
    "        for _ in set(other.domain) - set(self.domain):\n",
    "            self_t = self_t[..., np.newaxis]     \n",
    "        for _ in set(self.domain) - set(other.domain):\n",
    "            other_t = other_t[..., np.newaxis]\n",
    "        # And we need the new axes to be transposed to the correct location\n",
    "        old_order = list(other.domain) + list(set(self.domain) - set(other.domain)) \n",
    "        new_order = []\n",
    "        for v in new_dom:\n",
    "            new_order.append(old_order.index(v))\n",
    "        other_t = np.transpose(other_t, new_order)\n",
    "        # Now that the arrays are all set up, we can rely on numpy broadcasting to work out which numbers need to be added.\n",
    "        new_table = self_t + other_t\n",
    "        # The final step is to create the new outcomeSpace\n",
    "        new_outcomeSpace = self.outcomeSpace.copy()\n",
    "        new_outcomeSpace.update(other.outcomeSpace)\n",
    "        return self.__class__(tuple(new_dom), new_outcomeSpace, table=new_table)\n",
    "\n",
    "    def marginalize(self, var):\n",
    "        raise NotImplementedError(\"We don't usually marginalize log factors\")\n",
    "    def normalize(self):\n",
    "        raise NotImplementedError(\"We don't usually normalize log factors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Random Field (CRF)\n",
    "\n",
    "Let's start coding our CRF model. This model is a Markov network with two kinds of potentials:\n",
    "\n",
    "1. $\\phi_i(Y_i,x_i)$ is the score provided by the classifier for each character image $x_i$. Notice that $Y_i$ is a vector of scores, one for each class (letter).\n",
    "2. $\\phi(Y_i, Y_{i+1})$ is a measure of co-occurrence of consecutive letters. We will use the probability of occurrence of two successive letters in a dictionary. Therefore, we will need to estimate these quantities from the data.\n",
    "\n",
    "Our first task is to compute $P(y_i, y_{i+1})$. We need a large dataset of words. We can find several of them online. We will use one with [466k English words](https://github.com/dwyl/english-words).\n",
    "\n",
    "Let's start loading it into memory. It has one word per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open('data/words_alpha.txt', 'r')\n",
    "word_dataset = text_file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before computing the factors for our CRF, it is interesting to evaluate the accuracy of the RF model to recognise entire words correctly. Let's start by splitting the word dataset into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(W_train, W_test) = train_test_split(word_dataset, test_size=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function will compute the accuracy of the test set. We reserve the training set to compute $\\phi(Y_i, Y_{i+1})$ later on.\n",
    "\n",
    "We define a \"word accuracy\" as the percentage of words that were correctly recognised in the test set. To be correctly identified, the classifier must correctly recognise every letter. Therefore, we can expect a much lower accuracy for words that the one we got for individual letters.\n",
    "\n",
    "The next cell may take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_accuracy(words, letters_idx_dict):\n",
    "    acc = 0\n",
    "    for word in words:\n",
    "        word = word.rstrip()\n",
    "        w = make_word(word, letters_idx_dict, X_test)\n",
    "        pred_word = classify_random_forest(w)\n",
    "        if pred_word == word:\n",
    "            acc+=1\n",
    "    return acc/len(words)\n",
    "\n",
    "###################\n",
    "# Test code\n",
    "\n",
    "print(\"Word accuracy (Random Forest):\", word_accuracy(W_test, letters_idx_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we got an accuracy above 72% for words. It is considerably lower than the one we got for letters (around 97%). (For small dataset, we get ~42% for word accuracy).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise\n",
    "\n",
    "It is time to compute the probability of the occurrence of consecutive letters. We will use the training partition of the word list to compute these probabilities. Let's use Laplace smoothing to avoid zero probabilities.\n",
    "\n",
    "After computing each probability, we will store it in a \"transition\" factor $\\phi(Y_i, Y_{i+1})$. We will call this factor a \"transition\" factor since it quantifies the strength of transitioning between consecutive letters. This factor is quite large since it stores $26^2$ entries expressing the transition probabilities between all possible pairs of letters. \n",
    "\n",
    "Let's implement the function `create_transition_factor`. We will use log-probabilities to avoid underflow issues later. We have implemented part of the code for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomeSpace = {\n",
    "    'L':('a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'),\n",
    "    'L_next':('a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'),\n",
    "}\n",
    "\n",
    "def create_transition_factor(W_train, outcomeSpace, gamma=0.1):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `W_train`, a training dataset of words,\n",
    "    `outcomeSpace`, a dictionary with variables and their scope\n",
    "    `gamma`, a parameters to adjust the influence of the transition_factors\n",
    "    \n",
    "    Returns a transition factor that expresses hte correlation between two consecutive letters. The values are log-probabilities of occurrence of consecutive letters\n",
    "    \"\"\"\n",
    "    # Dictionary to map two-letter strings into occurrence counts\n",
    "    count_letters = {}\n",
    "    # Lets initialise the disctionary with zero counts\n",
    "    for two_letters in product(string.ascii_lowercase, string.ascii_lowercase):\n",
    "        count_letters[two_letters] = ... # TODO\n",
    "    # Now, we scan all words in the training set\n",
    "    for word in W_train:\n",
    "        # Remove the \\n character\n",
    "        word = word.rstrip()\n",
    "        # Finally, we scan every letter in a word and increment the occurrences\n",
    "        for i in range(len(word)-1):\n",
    "            count_letters[(word[i],word[i+1])] += ... # TODO\n",
    "    # Let's compute the total number of occurrence\n",
    "    total_letters = ... # TODO\n",
    "\n",
    "    tFactor = LogFactor(('L', 'L_next'), outcomeSpace)\n",
    "    for key, value in count_letters.items():\n",
    "        # Compute the empirical probabilities from the counts and apply smoothing with k = 1\n",
    "        prob = (value + 1) / (total_letters + len(string.ascii_lowercase) ** 2)\n",
    "        # Populate the factor with log-probabilities multiplied by the parameter gamma. Use math.log2\n",
    "        tFactor[key] = ... # TODO\n",
    "\n",
    "    return tFactor\n",
    "\n",
    "###########\n",
    "# Test code\n",
    "\n",
    "gamma = 0.1\n",
    "tFactor = create_transition_factor(W_train, outcomeSpace, gamma=gamma)\n",
    "\n",
    "print(\"gamma*logP(QU) = \", tFactor['q', 'u'])\n",
    "print(\"gamma*logP(QV) = \", tFactor['q', 'v'])\n",
    "\n",
    "print(\"P(QU) = \", 2**(tFactor['q', 'u']/gamma))\n",
    "print(\"P(QV) = \", 2**(tFactor['q', 'v']/gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "outcomeSpace = {\n",
    "    'L':('a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'),\n",
    "    'L_next':('a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'),\n",
    "}\n",
    "\n",
    "def create_transition_factor(W_train, outcomeSpace, gamma=0.1):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `W_train`, a training dataset of words,\n",
    "    `outcomeSpace`, a dictionary with variables and their scope\n",
    "    `gamma`, a parameters to adjust the influence of the transition_factors\n",
    "    \n",
    "    Returns a transition factor that expresses hte correlation between two consecutive letters. The values are log-probabilities of occurrence of consecutive letters\n",
    "    \"\"\"\n",
    "    # Dictionary to map two-letter strings into occurrence counts\n",
    "    count_letters = {}\n",
    "    # Lets initialise the disctionary with zero counts\n",
    "    for two_letters in product(string.ascii_lowercase, string.ascii_lowercase):\n",
    "        count_letters[two_letters] = 0\n",
    "    # Now, we scan all words in the training set\n",
    "    for word in W_train:\n",
    "        # Remove the \\n character\n",
    "        word = word.rstrip()\n",
    "        # Finally, we scan every letter in a word and increment the occurrences\n",
    "        for i in range(len(word)-1):\n",
    "            count_letters[(word[i],word[i+1])] += 1\n",
    "    # Let's compute the total number of occurrence\n",
    "    total_letters = sum(count_letters.values())\n",
    "\n",
    "    tFactor = LogFactor(('L', 'L_next'), outcomeSpace)\n",
    "    for key, value in count_letters.items():\n",
    "        # Compute the empirical probabilities from the counts and apply smoothing with k = 1\n",
    "        prob = (value + 1) / (total_letters + len(string.ascii_lowercase) ** 2)\n",
    "        # Populate the factor with log-probabilities multiplied by the parameter gamma. Use math.log2\n",
    "        tFactor[key] = gamma * math.log2(prob)\n",
    "\n",
    "    return tFactor\n",
    "\n",
    "###########\n",
    "# Test code\n",
    "\n",
    "gamma = 0.1\n",
    "tFactor = create_transition_factor(W_train, outcomeSpace, gamma=gamma)\n",
    "\n",
    "print(\"gamma*logP(QU) = \", tFactor['q', 'u'])\n",
    "print(\"gamma*logP(QV) = \", tFactor['q', 'v'])\n",
    "\n",
    "print(\"P(QU) = \", 2**(tFactor['q', 'u']/gamma))\n",
    "print(\"P(QV) = \", 2**(tFactor['q', 'v']/gamma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see an output similar to this one (the exact values will differ because of the random partitions of the data)\n",
    "```\n",
    "gamma*logP(QU) =  -0.9081278288352004\n",
    "gamma*logP(QV) =  -1.9976096051659948\n",
    "P(QU)= 0.0018416920553839273\n",
    "P(QV)= 9.696518368114745e-07\n",
    "```\n",
    "\n",
    "\n",
    "Similarly, we define a function to create the factors $\\phi_i(Y_i, x_i)$. We call these \"classification factors\". There will be one of those factors for each input character. \n",
    "\n",
    "## Exercise \n",
    "\n",
    "Let's implement the `classification_factors` function. This function receives a list of indexes with the position of the letters in the test set. For each letter, we will create one factor. This factor has the log-probabilities assigned by a classifier to each class. \n",
    "\n",
    "We have already started the code for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificationFactors(word_imgs):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `word_idx`, a list with test set indexes for each letter that form a word\n",
    "    \n",
    "    Returns a list of classification factors. Each factor has the probabilities assigned by a classifier to each letter and class\n",
    "    \"\"\"    \n",
    "    np.seterr(divide = 'ignore') \n",
    "    # use classifier.predict_log_proba to return the log-probabilities for each letter in word_idx\n",
    "    log_prob = ... # TODO 1 line with list comprehension\n",
    "    np.seterr(divide = 'warn') \n",
    "    # This variable has the list of factors we will return. We start with an empty list\n",
    "    factors = []    \n",
    "    for i in range(len(word_imgs)):\n",
    "        # Create an empty factor\n",
    "        f = LogFactor(('L',), outcomeSpace)\n",
    "        for letter in outcomeSpace['L_next']:\n",
    "            # Populate the factor with the log-probabilities in log_prob\n",
    "            f[letter] = ... # TODO 1 line\n",
    "        factors.append(f)\n",
    "    return factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "def classificationFactors(word_imgs):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `word_idx`, a list with test set indexes for each letter that form a word\n",
    "    \n",
    "    Returns a list of classification factors. Each factor has the probabilities assigned by a classifier to each letter and class\n",
    "    \"\"\"    \n",
    "    np.seterr(divide = 'ignore') \n",
    "    # Class classifier.predict_log_proba to return the log-probabilities for each letter in word_idx\n",
    "    log_prob = classifier.predict_log_proba([letter_img for letter_img in word_imgs])\n",
    "    np.seterr(divide = 'warn') \n",
    "    # This variable has the list of factors we will return. We start with an empty list\n",
    "    factors = []    \n",
    "    for i in range(len(word_imgs)):\n",
    "        # Create an empty factor\n",
    "        f = LogFactor(('L',), outcomeSpace)\n",
    "        for letter in outcomeSpace['L_next']:\n",
    "            # Populate the factor with the log-probabilities in log_prob\n",
    "            f[letter] = log_prob[i][ord(letter)-ord('a')]\n",
    "        factors.append(f)\n",
    "    return factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next three functions implement the Viterbi algorithm for CRFs. They are almost trivial adaptations of the same functions we implemented for Hidden Markov Models. For instance, instead of expecting transition and emission probabilities, these functions expect as input transition and classification factors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalRandomField():\n",
    "    def __init__(self, first_state, tFactor, variable_remap):\n",
    "        self.state = first_state\n",
    "        self.transition = tFactor\n",
    "\n",
    "        self.remap = variable_remap\n",
    "\n",
    "        self.history = []\n",
    "        self.prev_history = []\n",
    "\n",
    "    def viterbiStep(self, observationFactor):\n",
    "\n",
    "        # confirm that state and emission each have 1 variable \n",
    "        assert len(self.state.domain) == 1\n",
    "        assert len(self.transition.domain) == 2\n",
    "\n",
    "        # get state and evidence var names (to be marginalized and maximised out later)\n",
    "        state_var_name = self.state.domain[0]\n",
    "\n",
    "        # join with transition factor\n",
    "        f = self.state*self.transition\n",
    "\n",
    "        # maximize out old state vars, leaving only new state vars\n",
    "        f, prev = f.maximize(state_var_name, return_prev=True)\n",
    "        self.prev_history.append(prev)\n",
    "\n",
    "        # remap variable to it's original name\n",
    "        f.domain = tuple(self.remap[var] for var in f.domain)\n",
    "\n",
    "        # join observation factor with state factor\n",
    "        f = f*observationFactor\n",
    "\n",
    "        self.state = f \n",
    "\n",
    "        self.history.append(self.state)\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def viterbiBatch(self, observationFactorList):\n",
    "        '''\n",
    "        emissionEviList: A list of dictionaries, each dictionary containing the evidence for that timestep. \n",
    "                         Use `None` if no evidence for that timestep\n",
    "        '''\n",
    "        for observationFactor in observationFactorList:\n",
    "            # select evidence for this timestep\n",
    "            self.viterbiStep(observationFactor)\n",
    "        return self.history\n",
    "\n",
    "    def traceBack(self):\n",
    "        # get most likely outcome of final state\n",
    "        index = np.argmax(self.history[-1].table)\n",
    "        \n",
    "        # Go through \"prev_history\" in reverse\n",
    "        indexList = []\n",
    "        for prev in reversed(self.prev_history):\n",
    "            indexList.append(index)\n",
    "            index = prev[index]\n",
    "        indexList = reversed(indexList)\n",
    "\n",
    "        # translate the indicies into the outcomes they represent\n",
    "        mleList = []\n",
    "        stateVar = self.state.domain[0]\n",
    "        for idx in indexList:\n",
    "            mleList.append(self.state.outcomeSpace[stateVar][idx]) \n",
    "        return mleList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Implement the `classify_word_crf` function that uses the Viterbi algorithm to classify a word represented y a list of indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_word_crf(word_imgs, tFactor):\n",
    "    # Compute the observation factors for word_imgs (individual letter classification)\n",
    "    observationFactorList = classificationFactors(word_imgs)\n",
    "    \n",
    "    # Initialize a conditionalRandomField, using the first observation factor as the start factor\n",
    "    ... # TODO\n",
    "    # Use Viterbi algorithm to compute the time line\n",
    "    ... # TODO\n",
    "    # Obtain the MPE assigment by tracing back over the timeline\n",
    "    ... # TODO\n",
    "    return ... # TODO return the word\n",
    "\n",
    "\n",
    "##################\n",
    "# Test code\n",
    "\n",
    "rnd.seed(1)\n",
    "letters_idx_dict = letter_idxs()\n",
    "word_imgs = make_word(\"bayes\", letters_idx_dict, X_test)\n",
    "tFactor = create_transition_factor(W_train, outcomeSpace)\n",
    "print_word(word_imgs)\n",
    "print(classify_word_crf(word_imgs, tFactor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "def classify_word_crf(word_imgs, tFactor):\n",
    "    # Compute the classification factors for word_idx\n",
    "    observationFactorList = classificationFactors(word_imgs)\n",
    "    remap = {'L_next': 'L'}\n",
    "    crf_model = ConditionalRandomField(observationFactorList[0], tFactor, remap)\n",
    "    # Use Viterbi algorithm to compute the time line\n",
    "    crf_model.viterbiBatch(observationFactorList)\n",
    "    # Obtain the MPE assigment by tracing back over the timeline\n",
    "    mpe = crf_model.traceBack()\n",
    "    return ''.join(mpe)\n",
    "\n",
    "\n",
    "##################\n",
    "# Test code\n",
    "\n",
    "rnd.seed(1)\n",
    "letters_idx_dict = letter_idxs()\n",
    "word_imgs = make_word(\"bayes\", letters_idx_dict, X_test)\n",
    "tFactor = create_transition_factor(W_train, outcomeSpace)\n",
    "print_word(word_imgs)\n",
    "print(classify_word_crf(word_imgs, tFactor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finally in a position to use the CRF to classify words. The approach is to use the Viterbi algorithm and return the MPE assignment. The next function will perform this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the last step, we can compare the classification accuracy of Random Forest and Conditional Random Field. The next function will perform this task.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Implement `compare_acc` to compute the accuracy of RF and CRM in the test set partition of the word dataset.\n",
    "\n",
    "The next cell will take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_acc(W_train, W_test, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `W_train`, the training set partition of the word dataset.\n",
    "    `W_test`, the test set partition of the word dataset.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns a word predicted by the CRF model\n",
    "    \"\"\"    \n",
    "    acc_rf = 0\n",
    "    acc_crf = 0\n",
    "    # Compute the letter indexes by calling the appropriate function\n",
    "    letters_dict = ... # TODO\n",
    "    # Compute the transition factor \n",
    "    tFactor = create_transition_factor(W_train, outcomeSpace)\n",
    "    for word in W_test:\n",
    "        word = word.rstrip()\n",
    "        # Transform the word string in a list of letter indexes\n",
    "        word_imgs = ... # TODO\n",
    "        # Classify word_idx using the CRF model\n",
    "        word_crf = ... # TODO\n",
    "        # Classify word_idx using the RF model\n",
    "        word_rf = ... # TODO\n",
    "        if word_rf == word:\n",
    "            acc_rf += 1\n",
    "        if word_crf == word:\n",
    "            acc_crf += 1\n",
    "    return acc_rf/len(W_test), acc_crf/len(W_test)\n",
    "\n",
    "##################\n",
    "# Test code\n",
    "\n",
    "acc_rf, acc_crf = compare_acc(W_train, W_test, outcomeSpace)\n",
    "print(\"Accuracy RF:\", acc_rf)\n",
    "print(\"Accuracy CRF:\", acc_crf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "def compare_acc(W_train, W_test, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `W_train`, the training set partition of the word dataset.\n",
    "    `W_test`, the test set partition of the word dataset.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns a word predicted by the CRF model\n",
    "    \"\"\"    \n",
    "    acc_rf = 0\n",
    "    acc_crf = 0\n",
    "    # Compute the letter indexes by calling the appropriate function\n",
    "    letters_dict = letter_idxs()\n",
    "    # Compute the transition factor \n",
    "    tFactor = create_transition_factor(W_train, outcomeSpace)\n",
    "    for word in W_test:\n",
    "        word = word.rstrip()\n",
    "        # Transform the word string in a list of letter indexes\n",
    "        word_imgs = make_word(word, letters_dict, X_test)\n",
    "        # Classify word_idx using the CRF model\n",
    "        word_crf = classify_word_crf(word_imgs, tFactor)\n",
    "        # Classify word_idx using the RF model\n",
    "        word_rf = classify_random_forest(word_imgs)\n",
    "        if word_rf == word:\n",
    "            acc_rf += 1\n",
    "        if word_crf == word:\n",
    "            acc_crf += 1\n",
    "    return acc_rf/len(W_test), acc_crf/len(W_test)\n",
    "\n",
    "##################\n",
    "# Test code\n",
    "\n",
    "acc_rf, acc_crf = compare_acc(W_train, W_test, outcomeSpace)\n",
    "print(\"Accuracy RF:\", acc_rf)\n",
    "print(\"Accuracy CRF:\", acc_crf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is correct, you should see a significant improvement in classification accuracy. The CRF model should perform around 80% accuracy, while the RF has about 73%. (On the small dataset, accuracy should be about 42% and 60%). \n",
    "\n",
    "That is all for today. See you next week!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "198px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
